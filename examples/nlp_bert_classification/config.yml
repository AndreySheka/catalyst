model_params:
  model: DistilBertForSequenceClassification
  pretrained_model_name: distilbert-base-uncased
  num_classes: 6

dataset_params:
  path_to_data: './nlp_bert_classification/input'
  train_filename: "train.csv"
  validation_filename: "valid.csv"
  test_filename: "test.csv"
  text_field: "text"
  label_field: "label"
  max_sequence_length: 512

args:
  expdir: 'nlp_bert_classification'
  baselogdir: './logs/'

stages:
  data_params:
    batch_size: 24
    num_workers: 0

  state_params:
    main_metric: &reduce_metric loss
    minimize_metric: True

  criterion_params:
    criterion: CrossEntropyLoss

  optimizer_params:
    optimizer: Adam
    lr: 0.00005

  scheduler_params:
    scheduler: ReduceLROnPlateau

  callbacks_params:
    loss:
      callback: CriterionCallback
    accuracy:
      callback: AccuracyCallback
    optimizer:
      callback: OptimizerCallback
      accumulation_steps: 4
    scheduler:
      callback: SchedulerCallback
      reduce_metric: *reduce_metric
    saver:
      callback: CheckpointCallback

  stage1:
    state_params:
      num_epochs: 2

